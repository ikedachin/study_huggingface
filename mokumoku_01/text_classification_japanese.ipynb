{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://data-science-learning.com/archives/1085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import linecache\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ０1 データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie-enter',\n",
       " 'it-life-hack',\n",
       " 'kaden-channel',\n",
       " 'topic-news',\n",
       " 'livedoor-homme',\n",
       " 'peachy',\n",
       " 'sports-watch',\n",
       " 'dokujo-tsushin',\n",
       " 'smax']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = glob('./text/*')\n",
    "categories = [os.path.basename(x) for x in categories if not x.endswith('.txt')]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie-enter': 0,\n",
       " 'it-life-hack': 1,\n",
       " 'kaden-channel': 2,\n",
       " 'topic-news': 3,\n",
       " 'livedoor-homme': 4,\n",
       " 'peachy': 5,\n",
       " 'sports-watch': 6,\n",
       " 'dokujo-tsushin': 7,\n",
       " 'smax': 8}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# カテゴリをid化する辞書\n",
    "cat2id = {v:i for i, v in enumerate(categories)}\n",
    "cat2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2text(file):\n",
    "    with open(file, 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    text = ''\n",
    "    \n",
    "    for line in lines[2:]:\n",
    "        text += line.replace('\\n', '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-1 データをデータフレームに入れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie-enter\n",
      "it-life-hack\n",
      "kaden-channel\n",
      "topic-news\n",
      "livedoor-homme\n",
      "peachy\n",
      "sports-watch\n",
      "dokujo-tsushin\n",
      "smax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dic = {\n",
    "    'cat': [],\n",
    "    'text': [],\n",
    "    # 'cat_id': []\n",
    "}\n",
    "                       \n",
    "for cat in categories:\n",
    "    print(cat)\n",
    "    files = glob(f'./text/{cat}/*.txt')\n",
    "    for i, file in enumerate(files):\n",
    "        data_dic['cat'].append(cat)\n",
    "        data_dic['text'].append(file2text(file))\n",
    "        # data_dic['label'].append(cat2id[cat])\n",
    "\n",
    "dataset_df = pd.DataFrame(data_dic)\n",
    "dataset_df = dataset_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "dataset_df = dataset_df[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>インタビュー：植松晃士さん「黒目の印象を変えて、脱“変わり映えのしない女”」　美しくなりたい...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NTTドコモのLTEは地下鉄トンネル内で速いのか？都営新宿線にてスピードテストをしてみた【レ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>独女世代が共感「最後から二番目の恋」で描かれる「大人の孤独」派遣社員のジュンコさん(仮名・3...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GALAXY Noteでお絵かきしよう！ドット絵を打てる「Pixel Art editor」...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>超危険な日傘で注意したいNG行為／父親の愛を感じた体験談など−【ライフスタイル】週間ランキン...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>TOKIOの出演CMに海外から非難TOKIOが出演した「フードアクションニッポン」のCM「食...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>何だこのおもちゃみたいなの？ Googleロゴが子供のおもちゃみたいになっている理由前回紹介...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>めざせ、空き缶つぶし世界一！ストレス解消もできる「クシャッと」【iPhoneアプリ】清水幸子...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>香川、移籍のうわさを完全に否定　どうやら、香川真司にも「エアオファー」が届いていたようだ。　...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>【韓国ニュース】韓国に“日本人出入禁止”のネットカフェ 19日、韓国のネット掲示板には「ネッ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    インタビュー：植松晃士さん「黒目の印象を変えて、脱“変わり映えのしない女”」　美しくなりたい...      5\n",
       "1    NTTドコモのLTEは地下鉄トンネル内で速いのか？都営新宿線にてスピードテストをしてみた【レ...      8\n",
       "2    独女世代が共感「最後から二番目の恋」で描かれる「大人の孤独」派遣社員のジュンコさん(仮名・3...      7\n",
       "3    GALAXY Noteでお絵かきしよう！ドット絵を打てる「Pixel Art editor」...      8\n",
       "4    超危険な日傘で注意したいNG行為／父親の愛を感じた体験談など−【ライフスタイル】週間ランキン...      5\n",
       "..                                                 ...    ...\n",
       "195  TOKIOの出演CMに海外から非難TOKIOが出演した「フードアクションニッポン」のCM「食...      3\n",
       "196  何だこのおもちゃみたいなの？ Googleロゴが子供のおもちゃみたいになっている理由前回紹介...      1\n",
       "197  めざせ、空き缶つぶし世界一！ストレス解消もできる「クシャッと」【iPhoneアプリ】清水幸子...      8\n",
       "198  香川、移籍のうわさを完全に否定　どうやら、香川真司にも「エアオファー」が届いていたようだ。　...      6\n",
       "199  【韓国ニュース】韓国に“日本人出入禁止”のネットカフェ 19日、韓国のネット掲示板には「ネッ...      3\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df['label'] = dataset_df['cat'].map(cat2id)\n",
    "dataset_df = dataset_df[['text', 'label']]\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-2 datesets形式に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-3 データセットにトークン化したデータを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 13711, 897, 2778, 11191, 881, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('今日は暑かった')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x15c9bb910> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28146d66a3ca45cba026f59a1267d1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    MAX_LENGTH = 512\n",
    "    return tokenizer(examples[\"text\"], max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset = tokenized_dataset.remove_columns(['cat', 'column_to_remove2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splited_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "# splited_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splited_dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splited_dataset['train']['cat_id'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 modelの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer= AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbca39cb71384be1b324534703b839aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    MAX_LENGTH = 512\n",
    "    return tokenizer(examples[\"text\"], max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 160\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datacollatorについて調べること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, BertForSequenceClassification\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\", num_labels=len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-1 学習準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy':acc, 'f1':f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    # evaluation_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    learning_rate=2e-5,\n",
    "    use_cpu=True, # GPUを使用する場合はFalse\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# このままでは以下のエラーが出る\n",
    "# ValueError: You are trying to save a non contiguous tensor: `bert.encoder.layer.0.attention.self.query.weight` \n",
    "# which is not allowed. It either means you are trying to save tensors which are reference of each other in which\n",
    "# case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on \n",
    "# your tensor to pack it before saving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-2 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-1 エラー対策：Trainerクラスのsave_modelをオーバーライド"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "huggingfaceのtransformersとdatasetsなどのライブラリを使ってbertモデルの文章分類タスクの学習をしています。以下の学習条件で実行するとエラーが出ます。下記にその学習条件とエラーを示すので原因と対策を教えてください。\n",
    "\n",
    "学習条件\n",
    "def compute_metrics(pred):\n",
    "labels = pred.label_ids\n",
    "preds = pred.predictions.argmax(-1)\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "acc = accuracy_score(labels, preds)\n",
    "return {'accuracy':acc, 'f1':f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "output_dir=\"./results\",\n",
    "num_train_epochs=5,\n",
    "per_device_train_batch_size=8,\n",
    "per_device_eval_batch_size=8,\n",
    "weight_decay=0.01,\n",
    "# evaluation_strategy='epoch',\n",
    "eval_strategy='epoch',\n",
    "logging_strategy='epoch',\n",
    "save_strategy='epoch',\n",
    "save_total_limit=1,\n",
    "learning_rate=2e-5,\n",
    "use_cpu=True, # GPUを使用する場合はFalse\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "compute_metrics=compute_metrics,\n",
    "train_dataset=tokenized_dataset['train'],\n",
    "eval_dataset=tokenized_dataset['test'],\n",
    "tokenizer=tokenizer,\n",
    "data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "エラーコード\n",
    "ValueError: You are trying to save a non contiguous tensor: bert.encoder.layer.0.attention.self.query.weight which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call .contiguous() on your tensor to pack it before saving."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "このエラーは、保存しようとしているテンソルが非連続 (non-contiguous) であるために発生しています。これは、テンソルがメモリ内で連続していない場合に発生することがあります。この問題を解決するために、保存する前にテンソルを連続化する必要があります。\n",
    "\n",
    "以下に、Trainer クラスを拡張して、この問題を解決する方法を示します。\n",
    "\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "# Trainerクラスを拡張して保存する前にテンソルを連続化する\n",
    "class CustomTrainer(Trainer):\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "        \n",
    "        self.model = self.model.to('cpu')  # モデルをCPUに移動\n",
    "        \n",
    "        # すべてのテンソルを連続化する\n",
    "        for param in self.model.parameters():\n",
    "            if not param.is_contiguous():\n",
    "                param.data = param.data.contiguous()\n",
    "        \n",
    "        super().save_model(output_dir, _internal_call)\n",
    "\n",
    "# 新しいCustomTrainerクラスを使用\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "上記のコードでは、Trainer クラスを継承して CustomTrainer クラスを作成し、save_model メソッドをオーバーライドしています。その中で、すべてのテンソルが連続化されるようにしています。この方法で、非連続テンソルが原因のエラーを回避することができます。\n",
    "\n",
    "また、use_cpu オプションは TrainingArguments に存在しないため、削除するか、device オプションを適切に設定してください。GPUを使用しない場合、デフォルトでCPUが使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainerクラスを拡張して保存する前にテンソルを連続化する\n",
    "class CustomTrainer(Trainer):\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "        \n",
    "        self.model = self.model.to('cpu')  # モデルをCPUに移動\n",
    "        \n",
    "        # すべてのテンソルを連続化する\n",
    "        for param in self.model.parameters():\n",
    "            if not param.is_contiguous():\n",
    "                param.data = param.data.contiguous()\n",
    "        \n",
    "        super().save_model(output_dir, _internal_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新しいCustomTrainerクラスを使用\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1617ce944b645898ef5ec5f5be1f154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.496, 'grad_norm': 7.724746227264404, 'learning_rate': 1e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25180f9d0fa4c61a12597c41d1df17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4555186033248901, 'eval_accuracy': 0.725, 'eval_f1': 0.6996697746697746, 'eval_runtime': 7.1964, 'eval_samples_per_second': 5.558, 'eval_steps_per_second': 0.417, 'epoch': 1.0}\n",
      "{'loss': 1.1998, 'grad_norm': 6.1741108894348145, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e1088dfcaa4da79012930f35334267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3726389408111572, 'eval_accuracy': 0.75, 'eval_f1': 0.718482905982906, 'eval_runtime': 7.2774, 'eval_samples_per_second': 5.496, 'eval_steps_per_second': 0.412, 'epoch': 2.0}\n",
      "{'train_runtime': 380.3391, 'train_samples_per_second': 0.841, 'train_steps_per_second': 0.053, 'train_loss': 1.3479326725006104, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=1.3479326725006104, metrics={'train_runtime': 380.3391, 'train_samples_per_second': 0.841, 'train_steps_per_second': 0.053, 'total_flos': 84200829419520.0, 'train_loss': 1.3479326725006104, 'epoch': 2.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184031a9b1064402881f1ee5d498c512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 2, 1, 8, 0, 1, 2, 1, 6, 7, 3, 6, 6, 6, 3, 6, 0, 0, 1, 0, 3, 8, 8, 1, 6, 1, 0, 6, 0, 7, 7, 7, 0, 1, 5, 1, 1, 6, 3]\n"
     ]
    }
   ],
   "source": [
    "pred_result = trainer.predict(tokenized_dataset['test'], ignore_keys=['loss', 'last_hidden_state', 'hidden_states', 'attentions'])\n",
    "pred_label= pred_result.predictions.argmax(axis=1).tolist()\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "   movie-enter       0.71      1.00      0.83         5\n",
      "  it-life-hack       0.44      1.00      0.62         4\n",
      " kaden-channel       1.00      0.50      0.67         4\n",
      "    topic-news       1.00      1.00      1.00         4\n",
      "livedoor-homme       0.00      0.00      0.00         2\n",
      "        peachy       1.00      0.20      0.33         5\n",
      "  sports-watch       1.00      1.00      1.00         8\n",
      "dokujo-tsushin       1.00      1.00      1.00         4\n",
      "          smax       0.40      0.50      0.44         4\n",
      "\n",
      "      accuracy                           0.75        40\n",
      "     macro avg       0.73      0.69      0.65        40\n",
      "  weighted avg       0.80      0.75      0.72        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikedashinji/Desktop/study_huggingface/hug_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ikedashinji/Desktop/study_huggingface/hug_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ikedashinji/Desktop/study_huggingface/hug_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# print(classification_report(tokenized_dataset['test']['label'], pred_label))\n",
    "print(classification_report(tokenized_dataset['test']['label'], pred_label, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
